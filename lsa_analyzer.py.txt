import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import re

# Download required NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

class LSASemanticAnalyzer:
    def __init__(self, n_components=100):
        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')
        self.svd = TruncatedSVD(n_components=n_components)
        self.documents = []
        self.processed_docs = []
        self.tfidf_matrix = None
        self.lsa_matrix = None
        
    def preprocess_text(self, text):
        """Text preprocessing using techniques from Practical Work 1"""
        # Convert to lowercase
        text = text.lower()
        
        # Remove punctuation and numbers
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\d+', '', text)
        
        # Tokenization
        tokens = word_tokenize(text)
        
        # Remove stopwords
        stop_words = set(stopwords.words('english'))
        tokens = [token for token in tokens if token not in stop_words]
        
        # Lemmatization
        lemmatizer = WordNetLemmatizer()
        tokens = [lemmatizer.lemmatize(token) for token in tokens]
        
        # Remove short tokens
        tokens = [token for token in tokens if len(token) > 2]
        
        return ' '.join(tokens)
    
    def fit(self, documents):
        """Train LSA model on documents"""
        self.documents = documents
        
        # Preprocess all documents
        self.processed_docs = [self.preprocess_text(doc) for doc in documents]
        
        # Create TF-IDF matrix
        self.tfidf_matrix = self.vectorizer.fit_transform(self.processed_docs)
        
        # Apply LSA (SVD)
        self.lsa_matrix = self.svd.fit_transform(self.tfidf_matrix)
        
        print(f"LSA model trained with {len(documents)} documents")
        print(f"Original TF-IDF dimensions: {self.tfidf_matrix.shape}")
        print(f"LSA dimensions: {self.lsa_matrix.shape}")
        print(f"Explained variance ratio: {self.svd.explained_variance_ratio_.sum():.4f}")
        
    def document_similarity(self, doc1, doc2):
        """Calculate semantic similarity between two documents"""
        processed_doc1 = self.preprocess_text(doc1)
        processed_doc2 = self.preprocess_text(doc2)
        
        # Transform documents to LSA space
        doc1_tfidf = self.vectorizer.transform([processed_doc1])
        doc2_tfidf = self.vectorizer.transform([processed_doc2])
        
        doc1_lsa = self.svd.transform(doc1_tfidf)
        doc2_lsa = self.svd.transform(doc2_tfidf)
        
        # Calculate cosine similarity
        similarity = cosine_similarity(doc1_lsa, doc2_lsa)[0][0]
        return similarity
    
    def query_similarity(self, query, top_k=3):
        """Find most similar documents to a query"""
        processed_query = self.preprocess_text(query)
        
        # Transform query to LSA space
        query_tfidf = self.vectorizer.transform([processed_query])
        query_lsa = self.svd.transform(query_tfidf)
        
        # Calculate similarities with all documents
        similarities = cosine_similarity(query_lsa, self.lsa_matrix)[0]
        
        # Get top_k most similar documents
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            results.append({
                'document': self.documents[idx],
                'similarity': similarities[idx],
                'index': idx
            })
        
        return results
    
    def get_topic_terms(self, n_terms=10):
        """Get most important terms for each topic"""
        terms = self.vectorizer.get_feature_names_out()
        topic_terms = []
        
        for i, topic in enumerate(self.svd.components_):
            top_indices = topic.argsort()[-n_terms:][::-1]
            top_terms = [(terms[idx], topic[idx]) for idx in top_indices]
            topic_terms.append({
                'topic_id': i,
                'terms': top_terms
            })
        
        return topic_terms
    
    def transform_document(self, document):
        """Transform a single document to LSA space"""
        processed_doc = self.preprocess_text(document)
        doc_tfidf = self.vectorizer.transform([processed_doc])
        doc_lsa = self.svd.transform(doc_tfidf)
        return doc_lsa[0]

def main():
    # Sample documents for demonstration
    documents = [
        "Machine learning is a subset of artificial intelligence that focuses on algorithms.",
        "Deep learning uses neural networks with multiple layers for complex pattern recognition.",
        "Natural language processing helps computers understand and process human language.",
        "Computer vision enables machines to interpret and understand visual information.",
        "Data science involves extracting insights from structured and unstructured data.",
        "Python is a popular programming language for data analysis and machine learning.",
        "TensorFlow and PyTorch are widely used frameworks for deep learning projects.",
        "Supervised learning uses labeled datasets to train machine learning models.",
        "Unsupervised learning finds patterns in data without predefined labels.",
        "Reinforcement learning trains models through rewards and punishments system."
    ]
    
    # Initialize and train LSA model
    lsa_analyzer = LSASemanticAnalyzer(n_components=5)
    lsa_analyzer.fit(documents)
    
    print("\n" + "="*50)
    print("LSA SEMANTIC ANALYSIS DEMONSTRATION")
    print("="*50)
    
    # Show topic terms
    print("\nTOPIC TERMS:")
    topic_terms = lsa_analyzer.get_topic_terms(n_terms=5)
    for topic in topic_terms:
        print(f"Topic {topic['topic_id']}: {[term[0] for term in topic['terms']]}")
    
    # Test document similarity
    print("\nDOCUMENT SIMILARITY:")
    doc1 = "Machine learning algorithms learn from data"
    doc2 = "Artificial intelligence systems can make predictions"
    similarity = lsa_analyzer.document_similarity(doc1, doc2)
    print(f"Similarity between '{doc1}' and '{doc2}': {similarity:.4f}")
    
    # Test query similarity
    print("\nQUERY SIMILARITY SEARCH:")
    query = "neural networks and deep learning"
    results = lsa_analyzer.query_similarity(query, top_k=3)
    
    print(f"Query: '{query}'")
    print("Most similar documents:")
    for i, result in enumerate(results):
        print(f"{i+1}. Similarity: {result['similarity']:.4f}")
        print(f"   Document: {result['document']}")
    
    # Interactive mode
    print("\n" + "="*50)
    print("INTERACTIVE MODE")
    print("="*50)
    
    while True:
        print("\nOptions:")
        print("1. Search similar documents")
        print("2. Compare two documents")
        print("3. Exit")
        
        choice = input("\nEnter your choice (1-3): ").strip()
        
        if choice == '1':
            query = input("Enter your query: ")
            results = lsa_analyzer.query_similarity(query, top_k=3)
            print(f"\nTop 3 similar documents for '{query}':")
            for i, result in enumerate(results):
                print(f"{i+1}. (Similarity: {result['similarity']:.4f}) {result['document']}")
                
        elif choice == '2':
            doc1 = input("Enter first document: ")
            doc2 = input("Enter second document: ")
            similarity = lsa_analyzer.document_similarity(doc1, doc2)
            print(f"Semantic similarity: {similarity:.4f}")
            
        elif choice == '3':
            print("Goodbye!")
            break
        else:
            print("Invalid choice. Please try again.")

if __name__ == "__main__":
    main()